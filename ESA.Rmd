---
title: "EDS"
output: html_document
date: "2022-11-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(purrr)#to join all files
library(lubridate)# for duration of time
library(ModelMetrics)
library(pROC)
library(dplyr)
library(tidyr)
library(caret)
```


#joining four months in one sheet
```{r}
#bind all the files in this path together
file.list <- list.files(pattern='*.csv')

# need an id-column with the file-names
file.list <- setNames(file.list, file.list)

#want only sheet 4, skip 1 row and name id column month
bike <-map_df(file.list, function(x) read.csv(x))

str(bike)
```

```{r}
bike%>%group_by(member_casual)%>%
  summarise(count = n())

#70% is unbalanced data set and this cut is more than that
```

#Preparing columns for modeling
```{r}
#bike <- data.frame(read.csv("bo.csv"))
bike$rideable_type <- as.integer(factor(bike$rideable_type))
bike$member_casual <- as.integer(factor(bike$member_casual))
#make it to casual = 0, member= 1 
bike$member_casual<-ifelse(bike$member_casual==1,1,0)
# split started_at/ ended_at to date column and time column
# change start_date/end_date to date type
bike$start_time <- format(as.POSIXct(bike$started_at), format = "%H:%M") 
bike$end_time <- format(as.POSIXct(bike$ended_at), format = "%H:%M") 
bike$start_date <- as.Date(bike$started_at)
bike$end_date <- as.Date(bike$ended_at)
bike$Startedat = as.POSIXct(bike$started_at, format = "%Y-%m-%d %H:%M:%S")
bike$enddat = as.POSIXct(bike$ended_at, format = "%Y-%m-%d %H:%M:%S")
bike$Total_Time <- round(as.numeric(difftime(bike$enddat, bike$Startedat, units = "mins")),2)
bike$days <- format(bike$start_date, format = "%a")
#take the day to another column and delete the year and month
# make a plot to check if there is relationship between the days and the number of casual or members

```
# need to check for codes for times and days being rounded not bigger or lower

#make the time as a duration of mints

```{r}
res <- hm(bike$start_time)# format to 'hours:minutes'
bike$startingm <- hour(res)*60 + minute(res)

#one hot incoding of days
bike <-bike %>% mutate(value = 1)  %>% spread(days, value,  fill = 0 )
```


##Drop non-needed columns
```{r}
#selecting the columns we want
want <- c('member_casual','rideable_type', 'Total_Time', 'start_station_id', 'end_station_id', 'startingm','Fri','Mon', 'Sat', 'Sun', 'Thu', 'Tue', 'Wed')

df2 <- bike[,(names(bike) %in% want)]

#Delete the na 
df<-na.omit(df2)

#make member_casual to be the first column
colnames(df)
df <- df %>%
  select('member_casual', everything())
```

##carrolation metrix
```{r}
library(corrplot)

```

#find the most influnsing factors
```{r}
agesurvive = xtabs(~ member_casual + rideable_type, data = df)
chisq.test(agesurvive)
```

they are dependent factors

```{r}
agesurvive2 = xtabs(~ member_casual + Total_Time, data = df)
chisq.test(agesurvive2)
```

dependent due to low p-value

```{r}
agesurvive3 = xtabs(~ member_casual + start_station_id , data = df)
chisq.test(agesurvive3)
```

Interestingly very low p-value

```{r}
agesurvive4 = xtabs(~ member_casual + end_station_id , data = df)
chisq.test(agesurvive4)
```

dependent due to low p-value

```{r}
agesurvive5 = xtabs(~ member_casual + startingm , data = df)
chisq.test(agesurvive5)
```
dependent due to low p-value

# scale and devide the data
```{r}
scaleddf <- as.data.frame(scale(df[2:13], center = TRUE, scale = TRUE))
set.seed(321)
df_sample <- sample(2, nrow(scaleddf), replace=TRUE, prob=c(0.75, 0.25))


train_X <- scaleddf[df_sample==1, 1:12]
test_X <- scaleddf[df_sample==2, 1:12]


train_y <- df[df_sample==1, 1]
test_y <- df[df_sample==2, 1]

```


# Logistic model
```{r}
#deleted start_station_id and Wed coz the model says it is not much predictable 
customerLogit <- glm(train_y ~ end_station_id+Total_Time+rideable_type+ startingm*(Fri+Mon+Sat+Sun+Thu+Tue), data = train_X, family = "binomial")
summary(customerLogit)
```

# see 0.5 cut
```{r}
logitpredict.5 <- confupredict(customerLogit, newdata = test_X, type = "response") > 0.5
crossTable = table(logitpredict.5,test_y)
crossTable
```

Would it be possible to say that our one time customers that match the behaviors of annual customers are the false 1?

###The accuracy of the model of 0.5 cutoff :
```{r}
format(100*(crossTable[1,1]+crossTable[2,2])/sum(crossTable), digits=4)
```
###Checking the AUC
```{r,results='markup'}
prob=predict(customerLogit, type = "response")
e <- roc(train_y~prob)
auc(e) # area-under-curve 
plot(e)

```

